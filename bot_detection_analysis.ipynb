{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Bot Detection & Network Forensics\n",
    "\n",
    "## Project Overview\n",
    "This analysis project combines principles from **Data Visualization Theory**, **Statistics**, **Social Network Analysis**, and **Computational Linguistics** to identify automated behavior on social media.\n",
    "\n",
    "The methodology is derived from three foundational papers in the field:\n",
    "1.  **Varol et al. (2017):** *\"The Anatomy of a Social Bot\"* (Feature Engineering & Random Forests).\n",
    "2.  **Ferrara et al. (2016):** *\"The Rise of Social Bots\"* (Network Topology & Botometer).\n",
    "3.  **Complex Networks (2020):** *\"Coordinated Inauthentic Behavior\"* (Community Detection & Content Similarity).\n",
    "\n",
    "### Prerequisites\n",
    "Run the following cell to install necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib seaborn networkx scikit-learn textblob wordcloud scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from wordcloud import WordCloud\n",
    "from scipy.stats import entropy\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from math import pi\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "# Set Global Aesthetic\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Feature Engineering (Varol et al. Framework)\n",
    "**Objective:** Detect individual bots based on behavioral anomalies.\n",
    "\n",
    "### Methodology\n",
    "- **Shannon Entropy (Math):** $H(X) = -\\sum p(x) \\log p(x)$. used to measure linguistic complexity. Bots often use templates, resulting in low entropy.\n",
    "- **Inter-Arrival Time (Method):** We calculate the standard deviation of time between posts. Bots programmed on cron jobs show low variance.\n",
    "- **Random Forest (Model):** Used to determine feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Data Simulation (Feature Level) ---\n",
    "def generate_feature_data(n_users=60, posts_per_user=15):\n",
    "    data = []\n",
    "    users = []\n",
    "    \n",
    "    human_lexicon = [\"love\", \"hate\", \"tired\", \"politics\", \"coffee\", \"weekend\", \"work\", \"random\", \"thought\"]\n",
    "    bot_lexicon = [\"buy\", \"click\", \"deal\", \"now\", \"free\", \"limited\", \"offer\", \"crypto\", \"win\"]\n",
    "    start_time = datetime.now() - timedelta(days=30)\n",
    "\n",
    "    for i in range(n_users):\n",
    "        is_bot = i < (n_users * 0.25) # 25% Bots\n",
    "        user_id = f\"Bot_{i}\" if is_bot else f\"User_{i}\"\n",
    "        \n",
    "        # Bot: Low Variance Intervals. Human: Bursty (Poisson).\n",
    "        if is_bot:\n",
    "            intervals = [120 + random.randint(-2, 2) for _ in range(posts_per_user)]\n",
    "        else:\n",
    "            intervals = [int(random.expovariate(1/120)) + 1 for _ in range(posts_per_user)]\n",
    "            \n",
    "        current_time = start_time\n",
    "        for interval in intervals:\n",
    "            current_time += timedelta(minutes=interval)\n",
    "            \n",
    "            # Content Generation\n",
    "            if is_bot:\n",
    "                # Low Entropy Template\n",
    "                text = f\"Click here for {random.choice(bot_lexicon)} {random.choice(bot_lexicon)}\"\n",
    "            else:\n",
    "                # High Entropy Organic\n",
    "                text = \" \".join([random.choice(human_lexicon) for _ in range(random.randint(5, 12))])\n",
    "\n",
    "            data.append({\n",
    "                \"user\": user_id,\n",
    "                \"is_bot_ground_truth\": is_bot,\n",
    "                \"timestamp\": current_time,\n",
    "                \"text\": text,\n",
    "                \"followers\": random.randint(50, 500) if is_bot else random.randint(100, 2000)\n",
    "            })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# --- 2. Feature Calculation ---\n",
    "df_feat = generate_feature_data()\n",
    "\n",
    "def calc_entropy(text):\n",
    "    words = text.split()\n",
    "    if not words: return 0\n",
    "    counts = collections.Counter(words)\n",
    "    probs = [c / len(words) for c in counts.values()]\n",
    "    return entropy(probs, base=2)\n",
    "\n",
    "df_feat['entropy'] = df_feat['text'].apply(calc_entropy)\n",
    "\n",
    "# Aggregating per user\n",
    "user_stats = df_feat.groupby('user').agg({\n",
    "    'timestamp': list,\n",
    "    'entropy': 'mean',\n",
    "    'followers': 'first',\n",
    "    'is_bot_ground_truth': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "def get_iat_std(timestamps):\n",
    "    if len(timestamps) < 2: return 0\n",
    "    timestamps.sort()\n",
    "    diffs = [(t2 - t1).total_seconds()/60 for t1, t2 in zip(timestamps[:-1], timestamps[1:])]\n",
    "    return np.std(diffs)\n",
    "\n",
    "user_stats['iat_std'] = user_stats['timestamp'].apply(get_iat_std)\n",
    "\n",
    "# --- 3. Modeling & Visualization ---\n",
    "X = user_stats[['entropy', 'iat_std', 'followers']]\n",
    "y = user_stats['is_bot_ground_truth']\n",
    "rf = RandomForestClassifier(n_estimators=100).fit(X, y)\n",
    "importances = pd.DataFrame({'feature': X.columns, 'importance': rf.feature_importances_})\n",
    "\n",
    "# Plotting\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Scatter: The \"Math\" Test\n",
    "sns.scatterplot(x='entropy', y='iat_std', hue='is_bot_ground_truth', data=user_stats, \n",
    "                palette={True: 'red', False: 'blue'}, s=100, ax=ax1)\n",
    "ax1.set_title(\"Feature Space: Entropy vs. Temporal Variance\", fontsize=14)\n",
    "ax1.set_xlabel(\"Linguistic Entropy (Randomness)\")\n",
    "ax1.set_ylabel(\"Inter-Arrival Time Std Dev (Regularity)\")\n",
    "ax1.text(0.5, 5, \"Bots Cluster Here\\n(Low Entropy, Regular Timing)\", bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "\n",
    "# Bar: Feature Importance\n",
    "sns.barplot(x='importance', y='feature', data=importances, palette='viridis', ax=ax2)\n",
    "ax2.set_title(\"Random Forest Feature Importance\", fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Network Topology & Infiltration (Ferrara et al.)\n",
    "**Objective:** Detect bots attempting to infiltrate a network.\n",
    "\n",
    "### Methodology\n",
    "- **Reciprocity (Math):** $\\rho = \\frac{L^\\leftrightarrow}{L}$. Bots follow many, but few follow back.\n",
    "- **Clustering Coefficient:** Humans form triangles (communities); Bots form stars (broadcasters).\n",
    "- **The Radar Chart:** A visualization technique to compare the \"Digital DNA\" of bots vs humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Network Simulation ---\n",
    "def generate_network_data(n_humans=40, n_bots=10):\n",
    "    G = nx.DiGraph()\n",
    "    users = [f\"Human_{i}\" for i in range(n_humans)] + [f\"Bot_{i}\" for i in range(n_bots)]\n",
    "    \n",
    "    for uid in users:\n",
    "        if \"Bot\" in uid:\n",
    "            # Bots: Aggressive following, low reciprocity\n",
    "            targets = random.sample([u for u in users if u != uid], 20)\n",
    "            for t in targets:\n",
    "                G.add_edge(uid, t)\n",
    "        else:\n",
    "            # Humans: Reciprocal friendships\n",
    "            targets = random.sample([u for u in users if u != uid], 5)\n",
    "            for t in targets:\n",
    "                G.add_edge(uid, t)\n",
    "                if \"Human\" in t and random.random() < 0.7: G.add_edge(t, uid)\n",
    "    return G\n",
    "\n",
    "G_net = generate_network_data()\n",
    "\n",
    "# --- 2. Calculate Metrics ---\n",
    "metrics = []\n",
    "for node in G_net.nodes():\n",
    "    # Local Clustering for directed graph\n",
    "    clus = nx.clustering(G_net, node)\n",
    "    # Indegree vs Outdegree ratio\n",
    "    in_d = G_net.in_degree(node)\n",
    "    out_d = G_net.out_degree(node)\n",
    "    ratio = in_d / (out_d + 1)\n",
    "    \n",
    "    metrics.append({\n",
    "        'user': node,\n",
    "        'is_bot': \"Bot\" in node,\n",
    "        'clustering': clus,\n",
    "        'follow_ratio': ratio,\n",
    "        'centrality': nx.degree_centrality(G_net)[node]\n",
    "    })\n",
    "\n",
    "df_net = pd.DataFrame(metrics)\n",
    "\n",
    "# --- 3. Radar Chart Visualization ---\n",
    "# Normalize for Radar\n",
    "scaler = MinMaxScaler()\n",
    "cols = ['clustering', 'follow_ratio', 'centrality']\n",
    "df_norm = df_net.copy()\n",
    "df_norm[cols] = scaler.fit_transform(df_net[cols])\n",
    "\n",
    "avg_prof = df_norm.groupby('is_bot')[cols].mean()\n",
    "\n",
    "categories = list(avg_prof.columns)\n",
    "N = len(categories)\n",
    "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "angles += [angles[0]]\n",
    "\n",
    "fig = plt.figure(figsize=(18, 8))\n",
    "\n",
    "# Radar Plot\n",
    "ax1 = fig.add_subplot(1, 2, 1, polar=True)\n",
    "values_h = avg_prof.loc[False].tolist(); values_h += [values_h[0]]\n",
    "values_b = avg_prof.loc[True].tolist(); values_b += [values_b[0]]\n",
    "\n",
    "ax1.plot(angles, values_h, linewidth=2, label='Humans', color='blue')\n",
    "ax1.fill(angles, values_h, 'blue', alpha=0.1)\n",
    "ax1.plot(angles, values_b, linewidth=2, label='Bots', color='red')\n",
    "ax1.fill(angles, values_b, 'red', alpha=0.1)\n",
    "ax1.set_xticks(angles[:-1])\n",
    "ax1.set_xticklabels(categories)\n",
    "ax1.set_title(\"The 'BotOrNot' Radar Signature\", fontsize=15, pad=20)\n",
    "ax1.legend()\n",
    "\n",
    "# Network Graph\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "pos = nx.spring_layout(G_net, k=0.15)\n",
    "colors = ['red' if \"Bot\" in n else 'blue' for n in G_net.nodes()]\n",
    "nx.draw_networkx_nodes(G_net, pos, node_color=colors, node_size=100, ax=ax2)\n",
    "nx.draw_networkx_edges(G_net, pos, alpha=0.2, ax=ax2)\n",
    "ax2.set_title(\"Infiltration Topology (Red=Bots)\", fontsize=15)\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Coordinated Inauthentic Behavior (Complex Networks)\n",
    "**Objective:** Detect Bot Farms (groups of bots working together).\n",
    "\n",
    "### Methodology\n",
    "- **Projected Similarity Graph:** Instead of who follows whom, we map who *behaves* like whom.\n",
    "- **Cosine Similarity (Math):** Measures the angle between TF-IDF vectors of user content.\n",
    "- **Community Detection:** Identifying dense clusters in the similarity graph.\n",
    "- **Comparative Word Clouds:** Visualizing the specific narrative of the farm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Simulation: The Bot Farm ---\n",
    "def generate_farm_data(n_users=50):\n",
    "    users = [f\"User_{i}\" for i in range(n_users)]\n",
    "    farm_members = users[:15] # First 15 are the farm\n",
    "    data = []\n",
    "    \n",
    "    narratives = [\"Support the initiative\", \"The data is fake\", \"Buy crypto now\"]\n",
    "    \n",
    "    for i in range(300):\n",
    "        user = random.choice(users)\n",
    "        if user in farm_members:\n",
    "            # Farm: High Coordinated Content\n",
    "            base = random.choice(narratives)\n",
    "            text = base + random.choice([\"!\", \".\", \"!!\"])\n",
    "        else:\n",
    "            # Organic: Random Content\n",
    "            text = f\"Just having {random.choice(['lunch', 'fun', 'trouble'])} today\"\n",
    "        \n",
    "        data.append({'user': user, 'text': text})\n",
    "    return pd.DataFrame(data), farm_members\n",
    "\n",
    "df_farm, farm_members = generate_farm_data()\n",
    "\n",
    "# --- 2. Projected Similarity Graph ---\n",
    "# Group text by user\n",
    "user_docs = df_farm.groupby('user')['text'].apply(' '.join).reset_index()\n",
    "\n",
    "# TF-IDF & Cosine Sim\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "matrix = tfidf.fit_transform(user_docs['text'])\n",
    "sim_matrix = cosine_similarity(matrix)\n",
    "\n",
    "# Build Graph\n",
    "G_sim = nx.Graph()\n",
    "users = user_docs['user'].tolist()\n",
    "threshold = 0.5\n",
    "rows, cols = np.where(sim_matrix > threshold)\n",
    "edges = [(users[r], users[c]) for r, c in zip(rows, cols) if r != c]\n",
    "G_sim.add_edges_from(edges)\n",
    "\n",
    "# Detect Communities\n",
    "communities = list(nx.community.greedy_modularity_communities(G_sim))\n",
    "comm_map = {}\n",
    "for i, comm in enumerate(communities):\n",
    "    for n in comm: comm_map[n] = i\n",
    "\n",
    "# --- 3. Visualizations ---\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "# A. Network Projection\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "pos = nx.spring_layout(G_sim, k=0.2)\n",
    "colors = [comm_map.get(n, -1) for n in G_sim.nodes()]\n",
    "nx.draw_networkx_nodes(G_sim, pos, node_color=colors, cmap='tab10', node_size=200, ax=ax1)\n",
    "nx.draw_networkx_edges(G_sim, pos, alpha=0.3, ax=ax1)\n",
    "ax1.set_title(\"Projected Similarity Graph (Clusters = Coordination)\", fontsize=14)\n",
    "ax1.axis('off')\n",
    "\n",
    "# B. Similarity Heatmap\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "sorted_idx = np.argsort(list(comm_map.values())) # Sort by community to show blocks\n",
    "sorted_sim = sim_matrix[sorted_idx, :][:, sorted_idx]\n",
    "sns.heatmap(sorted_sim, cmap='viridis', ax=ax2)\n",
    "ax2.set_title(\"Adjacency Heatmap (Block Diagonal = Bot Farm)\", fontsize=14)\n",
    "\n",
    "# C. Comparative Word Clouds\n",
    "# Isolate texts\n",
    "farm_detected = list(communities[0]) # Assuming largest is farm\n",
    "organic_detected = list(itertools.chain.from_iterable(communities[1:]))\n",
    "\n",
    "text_farm = \" \".join(df_farm[df_farm['user'].isin(farm_detected)]['text'])\n",
    "text_org = \" \".join(df_farm[df_farm['user'].isin(organic_detected)]['text'])\n",
    "\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "wc_farm = WordCloud(background_color='black', colormap='Reds').generate(text_farm)\n",
    "ax3.imshow(wc_farm, interpolation='bilinear')\n",
    "ax3.set_title(\"Detected Bot Narrative\", fontsize=14, color='darkred')\n",
    "ax3.axis('off')\n",
    "\n",
    "ax4 = fig.add_subplot(2, 2, 4)\n",
    "wc_org = WordCloud(background_color='white', colormap='Blues').generate(text_org)\n",
    "ax4.imshow(wc_org, interpolation='bilinear')\n",
    "ax4.set_title(\"Organic Conversation\", fontsize=14, color='darkblue')\n",
    "ax4.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}