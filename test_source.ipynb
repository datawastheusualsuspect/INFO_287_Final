{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Report: Unsupervised Forensic Analysis of the \"Nutella\" Twitter Stream\n",
    "\n",
    "## 1. Research Problem\n",
    "**The Problem:** The digital landscape is compromised by \"Social Bots\"â€”algorithmic accounts designed to mimic human behavior. Modern bots have evolved from simple spammers into sophisticated \"cyborgs\" that infiltrate networks by following humans to solicit reciprocal follows. A major challenge in detecting them is the lack of \"Ground Truth\"; I do not have labels indicating who is a bot.\n",
    "\n",
    "**The Solution:** I propose a **Multi-Modal Forensic Pipeline**. I triangulate bot behavior using Computational Linguistics, Network Statistics, and Unsupervised Learning (K-Means) to mathematically separate organic users from automated actors.\n",
    "\n",
    "### Research Questions\n",
    "*   **RQ1 (The Turing Test):** Can I mathematically distinguish organic conversation from promotional/bot activity without labels?\n",
    "*   **RQ2 (The Infiltration Test):** Do specific clusters exhibit the \"Infiltration\" signature (High Friend/Follower ratio)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites & Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "from textblob import TextBlob\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from wordcloud import WordCloud\n",
    "from scipy.stats import entropy\n",
    "import collections\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "print(\"Environment Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Cleaning\n",
    "**Methodology:** I load the raw dataset (`result_Nutella.csv`) directly from the local project directory. Initial inspection reveals byte-string artifacts (e.g., `b'text'`) typical of raw API dumps. I apply a **Regex Cleaning Pipeline** to normalize the text for NLP analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'result_Nutella.csv'\n",
    "\n",
    "# --- Data Loading ---\n",
    "if os.path.exists(filename):\n",
    "    try:\n",
    "        # Load csv from the current directory\n",
    "        df = pd.read_csv(filename, on_bad_lines='skip')\n",
    "        print(f\"Success: Dataset loaded. Total Tweets: {len(df)}\")\n",
    "        \n",
    "        # --- INSPECT RAW DATA ---\n",
    "        print(\"\\n--- RAW DATA (First 10 Rows) ---\")\n",
    "        # Showing text column to visualize byte strings\n",
    "        print(df[['text']].head(10)) \n",
    "        \n",
    "        # --- Data Cleaning ---\n",
    "        def clean_tweet_text(text):\n",
    "            if pd.isna(text): return \"\"\n",
    "            text = str(text)\n",
    "            text = re.sub(r\"^b['\\\"]\", \"\", text) # Remove byte wrapper\n",
    "            text = re.sub(r\"['\\\"]$\", \"\", text)  # Remove trailing quote\n",
    "            text = re.sub(r'\\\\x[0-9a-fA-F]{2}', '', text) # Remove hex bytes\n",
    "            text = re.sub(r'\\\\u[0-9a-fA-F]{4}', '', text) # Remove unicode\n",
    "            text = re.sub(r'^RT @\\w+:', '', text) # Remove RT headers\n",
    "            return text\n",
    "\n",
    "        df['cleaned_text'] = df['text'].apply(clean_tweet_text)\n",
    "        \n",
    "        # --- INSPECT CLEANED DATA ---\n",
    "        print(\"\\n--- CLEANED DATA (First 10 Rows) ---\")\n",
    "        print(df[['text', 'cleaned_text']].head(10))\n",
    "        print(\"\\nData cleaning complete.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "else:\n",
    "    print(f\"Error: '{filename}' not found in current directory: {os.getcwd()}\")\n",
    "    print(\"Please ensure the .csv file is in the same folder as this notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering and Distribution Analysis\n",
    "**Methodology:** I engineer features to serve as statistical proxies for bot behavior.\n",
    "1.  **Linguistic Entropy:** ($H$) Measures complexity. Low entropy = repetitive templates.\n",
    "2.  **Infiltration Ratio:** ($\\frac{\\text{Friends}}{\\text{Followers} + 1}$). Measures aggressive following.\n",
    "3.  **Sentiment:** Measures emotional tone.\n",
    "4.  **Impact:** Log of Retweet counts.\n",
    "\n",
    "**Visual Inspection (EDA):** Before clustering, I visualize the distributions of these features using histograms. \n",
    "*   **Why?** To check for \"Bimodal Distributions\" (two humps). If the histograms show two distinct peaks, it suggests that the data naturally falls into two groups (e.g., Humans vs. Bots) even before we apply AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(text):\n",
    "    words = text.split()\n",
    "    if not words: return 0\n",
    "    counts = collections.Counter(words)\n",
    "    probs = [c / len(words) for c in counts.values()]\n",
    "    return entropy(probs, base=2)\n",
    "\n",
    "if 'cleaned_text' in df.columns:\n",
    "    # Calculations\n",
    "    df['entropy'] = df['cleaned_text'].apply(get_entropy)\n",
    "    df['sentiment'] = df['cleaned_text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    df['infiltration_ratio'] = df['friends'] / (df['followers'] + 1)\n",
    "    df['log_retwc'] = np.log1p(df['retwc'])\n",
    "\n",
    "    print(\"Feature Engineering Complete. Summary Statistics:\")\n",
    "    print(df[['entropy', 'sentiment', 'infiltration_ratio']].describe())\n",
    "\n",
    "    # --- VISUALIZATION: Feature Distributions ---\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    \n",
    "    # 1. Entropy\n",
    "    sns.histplot(df['entropy'], kde=True, ax=axes[0,0], color='skyblue')\n",
    "    axes[0,0].set_title(\"Distribution of Linguistic Entropy\")\n",
    "    \n",
    "    # 2. Sentiment\n",
    "    sns.histplot(df['sentiment'], kde=True, ax=axes[0,1], color='orange')\n",
    "    axes[0,1].set_title(\"Distribution of Sentiment Polarity\")\n",
    "    \n",
    "    # 3. Infiltration (Zoomed in to < 10 for visibility)\n",
    "    sns.histplot(df[df['infiltration_ratio'] < 10]['infiltration_ratio'], kde=True, ax=axes[1,0], color='green')\n",
    "    axes[1,0].set_title(\"Distribution of Infiltration Ratio (Zoomed < 10)\")\n",
    "    \n",
    "    # 4. Impact\n",
    "    sns.histplot(df['log_retwc'], kde=True, ax=axes[1,1], color='red')\n",
    "    axes[1,1].set_title(\"Distribution of Impact (Log Retweets)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Unsupervised Clustering (The Algorithm)\n",
    "\n",
    "**Why Clustering?**\n",
    "Because I do not have ground-truth labels (I don't know which tweets are bots), I cannot use Supervised Classification (like Random Forest). Instead, I use **Unsupervised Learning** to discover inherent structures in the data. The goal is to partition the tweets into two distinct groups based on statistical similarity.\n",
    "\n",
    "**The Mathematics (K-Means):**\n",
    "I employ the **K-Means Algorithm** ($k=2$). K-Means attempts to minimize the **Inertia** (Within-Cluster Sum of Squares). \n",
    "\n",
    "The objective function is:\n",
    "$$ J = \\sum_{j=1}^{k} \\sum_{i=1}^{n} ||x_i^{(j)} - \\mu_j||^2 $$\n",
    "\n",
    "Where:\n",
    "*   $||x_i - \\mu_j||^2$ is the **Euclidean Distance** between a data point $x_i$ and its cluster centroid $\\mu_j$.\n",
    "*   The algorithm iteratively assigns tweets to the nearest centroid and then recalculates the centroids until the position stabilizes.\n",
    "\n",
    "**The Approach (TF-IDF):**\n",
    "Every tweet contains the word \"Nutella\", so simple word counts are useless. I apply **TF-IDF**, which penalizes common words (IDF) and highlights unique vocabulary. I then combine these text vectors with the metadata features to perform the clustering.\n",
    "\n",
    "**Visualization:** To verify the clustering worked, I use **PCA (Principal Component Analysis)** to project the 504 dimensions down to 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cleaned_text' in df.columns:\n",
    "    # 1. TF-IDF Vectorization\n",
    "    tfidf = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "    text_vectors = tfidf.fit_transform(df['cleaned_text']).toarray()\n",
    "\n",
    "    # 2. Metadata Scaling\n",
    "    metadata_features = ['entropy', 'sentiment', 'infiltration_ratio', 'log_retwc']\n",
    "    scaler = MinMaxScaler()\n",
    "    X_meta = scaler.fit_transform(df[metadata_features].fillna(0))\n",
    "\n",
    "    # 3. Combine & Cluster (High Dimensional Space)\n",
    "    X_combined = np.hstack((X_meta, text_vectors))\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "    df['cluster'] = kmeans.fit_predict(X_combined)\n",
    "\n",
    "    print(f\"Clustering Complete. Found {len(df['cluster'].unique())} groups.\")\n",
    "    \n",
    "    # 4. PCA Projection (Visualizing the separation in 2D)\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_combined)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=df['cluster'], palette='viridis', alpha=0.6)\n",
    "    plt.title(\"PCA Projection of Clusters (2D Visualization of 504 Dimensions)\")\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Result 1: Summary Statistics\n",
    "**Approach:** I visualize the magnitude of the dataset and the temporal flow.\n",
    "**Findings:** The Bar Chart shows the balance between the two detected groups. The Line Chart reveals if activity was continuous or bursty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if 'cluster' in df.columns:\n",
    "    try:\n",
    "        # Pre-calc Time\n",
    "        df['created'] = pd.to_datetime(df['created'], errors='coerce')\n",
    "        df = df.dropna(subset=['created'])\n",
    "        df['minute'] = df['created'].dt.floor('T')\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "        # Bar Chart\n",
    "        sns.countplot(x='cluster', data=df, palette='viridis', ax=axes[0])\n",
    "        axes[0].set_title(\"Cluster Distribution (Count)\")\n",
    "\n",
    "        # Line Chart\n",
    "        time_data = df.groupby('minute').size()\n",
    "        time_data.plot(kind='line', ax=axes[1], color='#c0392b', lw=2)\n",
    "        axes[1].set_title(\"Temporal Fluctuation (Tweets/Min)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"Output suppressed due to error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Result 2: Temporal Forensics (Heatmap)\n",
    "**Approach:** I plot a two-way frequency heatmap (Hour vs. Minute) to detect robotic scheduling.\n",
    "**Findings:** I look for **Vertical Stripes**. If activity spikes at exactly `:00`, `:15`, or `:30` across different hours, it indicates Cron Job automation (False Positive check: ensure stripes persist across multiple hours)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cluster' in df.columns:\n",
    "    df['hour'] = df['created'].dt.hour\n",
    "    df['min_int'] = df['created'].dt.minute\n",
    "\n",
    "    pivot_table = df.pivot_table(index='hour', columns='min_int', values='text', aggfunc='count', fill_value=0)\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    sns.heatmap(pivot_table, cmap='YlGnBu', cbar_kws={'label': 'Frequency'})\n",
    "    plt.title(\"Temporal Heatmap (Hour vs Minute) - Detection of Cron Jobs\")\n",
    "    plt.xlabel(\"Minute of Hour\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Result 3: Behavioral DNA (Radar & Scatter)\n",
    "**Approach:** \n",
    "1.  **Radar Chart:** visualizes the average profile of each cluster (Central Tendency).\n",
    "2.  **Scatter Plot:** A **Visual Test for Outliers**. I plot Entropy vs. Infiltration.\n",
    "\n",
    "**Findings:** \n",
    "*   **The Bot Pattern:** Look for the cluster in the **Top-Left Quadrant** of the scatter plot (Low Entropy, High Infiltration). These are users who follow aggressively but speak robotically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cluster' in df.columns:\n",
    "    from math import pi\n",
    "    fig = plt.figure(figsize=(18, 8))\n",
    "\n",
    "    # Radar Chart\n",
    "    ax1 = fig.add_subplot(1, 2, 1, polar=True)\n",
    "    cluster_means = df.groupby('cluster')[metadata_features].mean()\n",
    "    cluster_means_norm = (cluster_means - cluster_means.min()) / (cluster_means.max() - cluster_means.min())\n",
    "    angles = [n / float(len(metadata_features)) * 2 * pi for n in range(len(metadata_features))]\n",
    "    angles += [angles[0]]\n",
    "\n",
    "    for i in range(2):\n",
    "        vals = cluster_means_norm.loc[i].tolist()\n",
    "        vals += [vals[0]]\n",
    "        ax1.plot(angles, vals, linewidth=2, label=f'Cluster {i}')\n",
    "        ax1.fill(angles, vals, alpha=0.1)\n",
    "    ax1.set_xticks(angles[:-1])\n",
    "    ax1.set_xticklabels(metadata_features)\n",
    "    ax1.set_title(\"Cluster Profiles (Radar)\")\n",
    "    ax1.legend(loc='upper right', bbox_to_anchor=(1.3, 0.1))\n",
    "\n",
    "    # Scatter Plot\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    sns.scatterplot(data=df, x='entropy', y='infiltration_ratio', hue='cluster', palette='viridis', s=100, alpha=0.7, ax=ax2)\n",
    "    ax2.set_title(\"Anomaly Detection: Entropy vs. Infiltration\")\n",
    "    ax2.set_ylabel(\"Infiltration Ratio (Friends/Followers)\")\n",
    "    ax2.set_ylim(0, 10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Result 4: Content Forensics (Word Clouds)\n",
    "**Approach:** I generate Word Clouds weighted by **TF-IDF Scores** (not raw counts).\n",
    "**Findings:** This allows me to validate the clusters qualitatively. If one cluster emphasizes organic words (\"Breakfast\", \"Love\") and the other emphasizes promotional words (\"Win\", \"Free\"), the unsupervised model has successfully identified the threat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cluster' in df.columns:\n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    c0_idx = df.index[df['cluster'] == 0].tolist()\n",
    "    c1_idx = df.index[df['cluster'] == 1].tolist()\n",
    "\n",
    "    # Sum TF-IDF scores\n",
    "    c0_freqs = dict(zip(feature_names, text_vectors[c0_idx].sum(axis=0)))\n",
    "    c1_freqs = dict(zip(feature_names, text_vectors[c1_idx].sum(axis=0)))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "    wc0 = WordCloud(background_color='white', colormap='Blues').generate_from_frequencies(c0_freqs)\n",
    "    axes[0].imshow(wc0, interpolation='bilinear')\n",
    "    axes[0].axis('off')\n",
    "    axes[0].set_title(\"Cluster 0 Narrative (TF-IDF Weighted)\")\n",
    "\n",
    "    wc1 = WordCloud(background_color='black', colormap='Reds').generate_from_frequencies(c1_freqs)\n",
    "    axes[1].imshow(wc1, interpolation='bilinear')\n",
    "    axes[1].axis('off')\n",
    "    axes[1].set_title(\"Cluster 1 Narrative (TF-IDF Weighted)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Result 5: Quantitative Term Analysis (Top 10 Words)\n",
    "**Approach:** While the Word Cloud provides a general overview, I explicitly extract the **Top 10 Highest Weighted Terms** for each cluster based on TF-IDF sum scores.\n",
    "\n",
    "**Findings:** This provides a concrete list of vocabulary. If the \"Inauthentic\" cluster's top terms are identical viral hashtags or calls to action (e.g., \"RT\", \"Follow\"), while the \"Organic\" cluster's terms are conversational, this quantifies the narrative divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cluster' in df.columns:\n",
    "    # Sort dictionaries by value to find top 10\n",
    "    top10_c0 = sorted(c0_freqs.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    top10_c1 = sorted(c1_freqs.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "    # Convert to DataFrames for plotting\n",
    "    df_c0 = pd.DataFrame(top10_c0, columns=['term', 'score'])\n",
    "    df_c1 = pd.DataFrame(top10_c1, columns=['term', 'score'])\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "    # Cluster 0 Bar Chart\n",
    "    sns.barplot(x='score', y='term', data=df_c0, ax=axes[0], palette='Blues_r')\n",
    "    axes[0].set_title(\"Top 10 Terms: Cluster 0 (Organic)\")\n",
    "    axes[0].set_xlabel(\"Cumulative TF-IDF Score\")\n",
    "\n",
    "    # Cluster 1 Bar Chart\n",
    "    sns.barplot(x='score', y='term', data=df_c1, ax=axes[1], palette='Reds_r')\n",
    "    axes[1].set_title(\"Top 10 Terms: Cluster 1 (Inauthentic/Bot)\")\n",
    "    axes[1].set_xlabel(\"Cumulative TF-IDF Score\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion and Limitations\n",
    "\n",
    "In conclusion, this unsupervised forensic analysis successfully isolated a coordinated group of inauthentic actors within the Nutella conversation. As visualized below, the 'Inauthentic' cluster (Cluster 1) is statistically distinct: its members aggressively follow others without being followed back (High Infiltration) and utilize a highly repetitive, viral vocabulary (Low Entropy). The contrast between the organic 'breakfast' conversation and the robotic 'giveaway' spam validates the efficacy of the K-Means approach even in the absence of ground-truth labels.\n",
    "\n",
    "**Limitations:**\n",
    "1.  **No Ground Truth:** I cannot calculate Precision/Recall without labels.\n",
    "2.  **Short Time Window:** The heatmap may show false patterns due to the limited 3-hour duration of the dataset.\n",
    "3.  **Proxy Metrics:** I used Infiltration Ratio as a substitute for Degree Centrality due to missing User IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composite Conclusion Visualization\n",
    "if 'cluster' in df.columns:\n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    plt.suptitle(\"CONCLUSION: The 'Smoking Gun' Evidence\", fontsize=16, weight='bold')\n",
    "\n",
    "    # 1. The Smoking Gun: Scatter Plot\n",
    "    ax1 = fig.add_subplot(2, 1, 1)\n",
    "    sns.scatterplot(data=df, x='entropy', y='infiltration_ratio', hue='cluster', palette='viridis', s=100, alpha=0.7, ax=ax1)\n",
    "    ax1.set_title(\"The Separation: Bots (Top-Left) vs. Humans (Bottom-Right)\", fontsize=14)\n",
    "    ax1.set_xlabel(\"Linguistic Entropy (Complexity)\")\n",
    "    ax1.set_ylabel(\"Infiltration Ratio\")\n",
    "    ax1.set_ylim(0, 10)\n",
    "    \n",
    "    # 2. The Content Proof: Top Terms Comparison\n",
    "    ax2 = fig.add_subplot(2, 2, 3)\n",
    "    sns.barplot(x='score', y='term', data=df_c0, ax=ax2, palette='Blues_r')\n",
    "    ax2.set_title(\"Cluster 0: Organic Vocabulary\")\n",
    "\n",
    "    ax3 = fig.add_subplot(2, 2, 4)\n",
    "    sns.barplot(x='score', y='term', data=df_c1, ax=ax3, palette='Reds_r')\n",
    "    ax3.set_title(\"Cluster 1: Inauthentic Vocabulary\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}